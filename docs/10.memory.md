# PCAI 系統架構說明：記憶與對話 (Memory & Session)

本文件說明 PCAI 系統中關於對話狀態管理 (Session) 與長期/短期記憶 (Memory) 的核心架構。

## 1. 對話狀態 (Session)

Session 負責儲存當前的對話上下文 (Context Window)，讓 Agent 能夠進行連續對話。

### 核心組件
- **結構定義**: `internal/history/session.go`
    - `Session` struct 包含 `ID` (字串), `Messages` (對話陣列), `LastUpdate` (時間戳)。
- **儲存位置**: `botmemory/history/*.json`
    - 每個 Session 存為一個 JSON 檔案。
    - CLI 模式預設讀取最新修改的 Session (`LoadLatestSession`)。
    - Telegram 模式為每個 User ID 建立獨立 Session (`telegram_{UserID}.json`)。

### 運作流程 (Telegram 範例)
1.  **用戶傳訊**: `AgentAdapter` 接收訊息。
2.  **載入/建立**: 呼叫 `history.LoadSession("telegram_" + userID)`。若無則建立新檔。
3.  **對話**: 將用戶訊息 append 到 `Messages`，送入 LLM。
4.  **回應**: 將 LLM 回應 append 到 `Messages`。
5.  **存檔**: 呼叫 `history.SaveSession()` 寫回 JSON。
6.  **歸納 (RAG)**: 背景執行 `history.CheckAndSummarize`，若對話過長或閒置過久，會觸發歸納機制（目前主要依賴時間或手動觸發）。

---

## 2. 記憶系統 (Memory)

記憶系統負責長期保存知識、事實與重要資訊，並支援語意搜尋 (Semantic Search)。

### 核心組件
1.  **Manager (`internal/memory/manager.go`)**:
    - **Vector Store**: 維護一個記憶條目 (`Entry`) 的列表。
    - **Embedding**: 使用 Ollama (`mxbai-embed-large`) 將文字轉為向量。
    - **Persistence**: 存於 `botmemory/knowledge/pcai_memory.sqlite` (包含向量與 FTS5 全文索引)。
    - **Search**: 支援 Cosine Similarity + 關鍵字加權混合搜尋。

2.  **Controller (`internal/memory/controller.go`)**:
    - **Router**: 負責協調記憶的寫入流程。
    - **Integration**: 整合 `Manager` (長期儲存) 與 `PendingStore` (待確認區)。

3.  **Pending Store (`internal/memory/pending_store.go`)**:
    - **Buffer**: 暫存 AI 提取的記憶，等待用戶確認。
    - **TTL**: 預設 24 小時過期。
    - **Confirmation**: 用戶透過 `memory_confirm` 工具批准後，才寫入 Manager。

4.  **Skills (`internal/memory/skills.go`)**:
    - **Extraction**: 定義如何從對話中提取記憶（雖程式碼有框架，目前主要依賴 LLM Tool Use 直接呼叫 `memory_save`）。

### 儲存位置
- **向量庫與索引**: `botmemory/knowledge/pcai_memory.sqlite`
  - **功能角色**：此為系統大腦的「**超級目錄與高速檢索庫（機器專用）**」。當 `MEMORY.md` 變更時，系統會自動在背景將文字切塊、加上 CJK 空白字元並算出向量存放於此。
  - **雙重引擎**：它同時包含 `chunks_fts` 虛擬資料表（提供毫秒級的 `BM25` 關鍵字全文檢索）與 `embeddings` 表（提供語意相似度的 `Vector Search` 檢索）。若此檔案遺失，系統會在重啟時依據 `MEMORY.md` 自動重建，故不需手動備份。
- **長期記憶日誌**: `botmemory/knowledge/MEMORY.md` (與 `memory/*.md`) (人類閱讀用，核心事實基準，容量無上限)
- **短期記憶 (SQLite)**: `pcai.db` -> table `short_term_memory` (用於晨間簡報、暫存對話摘要)。

### 運作流程 (記憶寫入)
1.  **觸發**: 用戶說 "記住我喜歡藍色"。
2.  **工具呼叫**: Agent 呼叫 `memory_save` 工具。
3.  **暫存**: 內容存入 `PendingStore`，回傳 Pending ID。
4.  **通知**: Agent 告知用戶 "已暫存，請確認"。
5.  **確認**: 用戶說 "確認" 或呼叫 `memory_confirm`。
6.  **寫入**: Controller 將內容移入 `Manager` (計算向量並存檔) 並追加到 `MEMORY.md`。

## 3. 記憶庫與 RAG (檢索增強生成) 機制

根據專案的架構設計，記憶庫（RAG）主要分為 **寫入時機** 與 **搜尋與使用時機** 兩個主要部分。本文件詳細說明這兩大部分的運作流程。

### 3.1 RAG 何時會被寫入？

記憶庫目前主要有三種寫入機制，其中最重要的機制改為**「先暫存後確認」**：

#### 1. AI 手動記憶（需要使用者確認）
- **關連檔案**：`docs/rag_write_confirmation.md`, `internal/memory/pending_store.go`, `tools/memory_save.go`
- **時機**：當你在對話中明確要求 AI 記住某些資訊（例如：「記住我喜歡喝咖啡」），AI 會呼叫 `memory_save` 工具。
- **寫入流程**：
  - 資料**不會立刻永久寫入**，而是進入 `PendingStore` 的未確認佇列（30 分鐘自動過期）。
  - AI 接著會反問你：「準備記住XXX，確認嗎？」
  - 只有當你回答「確認」時，AI 呼叫 `memory_confirm` 工具，才會正式寫入 `botmemory/knowledge/MEMORY.md`，並同步更新 SQLite 內的向量索引 (Vector DB)。

#### 2. 對話自動紀錄（每日日誌儲存）
- **關連檔案**：`internal/history/history.go` (函式 `CheckAndSummarize`)
- **時機**：每次你送出訊息時（當系統啟動 `GlobalMemoryToolKit` 時）。
- **寫入流程**：使用者的發言會被即時寫進今日專屬的日誌檔 `botmemory/memory/YYYY-MM-DD.md`。這些紀錄之後會被分塊（Chunking）與向量嵌入（Embedding）並索引到 SQLite 資料庫。

#### 3. 背景閒置歸納（舊版歷史濃縮備用機制）
- **關連檔案**：`internal/history/history.go` (函式 `CheckAndSummarize`)
- **時機**：當對話階段（Session）超時閒置逾 1 小時後自動觸發。
- **寫入流程**：系統會將這 1 小時內的對話重點提煉（精煉出3-5個關鍵字），儲存至長時間備份文件 `botmemory/history/auto_summaries.md`。

---

### 3.2 RAG 何時會用來搜尋/作為回答參考？

記憶檢索直接綁定在 **AI 回答生成前的攔截層**，分為**短期動態比對**與**長期向量搜尋**，讓它能不被察覺地自然提取背景知識：

#### 1. 提問前的動態搜尋注入 (自動 RAG Context)
- **關連檔案**：`internal/agent/memory_context.go` (函式 `BuildMemorySearchFunc`)
- **時機**：每次發送提問前，系統都會預先拿你的「問題字詞」作為 Query 去執行 RAG 搜索，然後**混入這次發給 LLM 的隱含 Prompt 中**。
  - **短期記憶快速比對**：若你的問題出現了特定關鍵字（如：`天氣`、`行事曆`、`信件`等），系統會直接從 SQLite 短期資料表抓近 3 筆對應資料。
  - **長期記憶混合過濾（向量 + 關鍵字 BM25）**：將使用者的問題拿去資料庫做關聯強度比對（Threshold 需超過 `0.05`），最多取回前 3 筆切塊的記憶文字。
    - **【容量無上限與 Context 限制說明】**：`MEMORY.md` 檔案本身的內容是**不限字數的（無限容量）**。當存入大量對話時，底層 SQLite 索引會將其切塊 (Chunking)。為避免過長的上下文導致 LLM 迷失重點 (Lost in the Middle) 或超過 Token 限制，系統在擷取「最關聯的 3 筆記憶」發送給 AI 時，**嚴格限制每筆記憶區塊最多擷取 1500 個中文字元 (Runes)**。這樣既能保存無盡過往，又能維持 AI 回答精準度。
- **注入結果**：一旦搜尋到高度關聯的背景知識，系統會將這段記憶預設為**「【最高優先級警告】這份背景代表實際的生活...你必須絕對無條件信任」**的嚴格前綴指令附加在 Prompt 開頭，確保 AI 根據你的真實背景回答。

#### 2. 全域提示詞附帶 (靜態 System Prompt)
- **關連檔案**：`internal/history/rag.go` (函式 `GetRAGEnhancedPrompt`)
- **時機**：如果系統有長遠的核心 Bootstrap 記憶，或是單純沒找到高度關聯片段時作為兜底（Fallback），系統會直接把 `MEMORY.md` 內容（最高取到 4000 字）無條件當成系統提示詞的前綴「以下是你的長期記憶，可用於回答問題：」並塞入模型的上下文中。

## 4. 架構圖示

```mermaid
graph TD
    User[User (Telegram/CLI)] --> Adapter[Agent Adapter]
    Adapter --> Session[Session Manager]
    Session <--> HistFile[(history/*.json)]
    
    Adapter --> Agent[AI Agent]
    Agent --> Tools[Tool Registry]
    
    subgraph Memory System
        Tools --> MemSave[memory_save Tool]
        Tools --> MemConfirm[memory_confirm Tool]
        Tools --> MemSearch[memory_search Tool]
        
        MemSave --> Pending[Pending Store (RAM)]
        MemConfirm --> Pending
        MemConfirm --> Controller
        
        Controller --> Manager[Memory Manager]
        Manager <--> VecDB[(pcai_memory.sqlite)]
        Manager <--> Markdown[(MEMORY.md)]
    end
    
    subgraph SearchPipeline[搜尋管線]
        MemSearch --> Adaptive{自適應檢索}
        Adaptive -->|需要搜尋| Hybrid[BM25 + Vector 混合搜尋]
        Adaptive -->|跳過| Skip[返回空]
        Hybrid --> RRF[RRF 融合]
        RRF --> Scoring[多階段評分管線]
        Scoring --> NoiseFilter[噪音過濾]
        NoiseFilter --> MMR[MMR 去重]
    end
    
    subgraph ShortTerm
        Adapter --> SQLite[(pcai.db)]
    end
```

---

## 5. 多階段評分管線 (memory-lancedb-pro)

> 移植自 [memory-lancedb-pro](https://github.com/win4r/memory-lancedb-pro)，在 BM25 + Vector 混合融合之後，依序套用多個評分階段以提升搜尋品質。

### 核心組件
- **程式檔案**: `internal/memory/scoring.go`
- **配置結構**: `RetrievalConfig` (定義於 `types.go`)

### 評分流程

```
Query → Vector Search + BM25 FTS → RRF 融合 → 多階段管線 → 最終結果
```

| 階段 | 公式 | 效果 |
|------|------|------|
| **RRF 融合** | 向量分數為基底，BM25 命中加成 15% | 確保關鍵字精確匹配得到加分 |
| **新鮮度加成** | `score += exp(-ageDays / 14) * 0.10` | 較新記憶得到加分 (14 天半衰期) |
| **重要度加權** | `score *= (0.7 + 0.3 * importance)` | importance=1.0 → ×1.0, 0.5 → ×0.85 |
| **長度正規化** | `score *= 1 / (1 + 0.5 * log2(len/500))` | 防止長條目因關鍵字密度主導 |
| **時間衰減** | `score *= 0.5 + 0.5 * exp(-ageDays / 60)` | 舊記憶漸失權重，底線 0.5× |
| **硬性閾值** | 丟棄 `score < 0.35` | 移除無關結果 |
| **噪音過濾** | 正則過濾低品質內容 | 移除打招呼/拒絕回應 |
| **MMR 去重** | 餘弦相似度 > 0.85 降級 | 防止近似重複占滿前 K 名 |

### 預設配置

```json
{
  "retrieval": {
    "vectorWeight": 0.7,
    "bm25Weight": 0.3,
    "minScore": 0.3,
    "hardMinScore": 0.35,
    "recencyHalfLifeDays": 14,
    "recencyWeight": 0.10,
    "lengthNormAnchor": 500,
    "timeDecayHalfLifeDays": 60,
    "mmrThreshold": 0.85,
    "filterNoise": true
  }
}
```

---

## 6. 噪音過濾 (Noise Filter)

- **程式檔案**: `internal/memory/noise_filter.go`
- **用途**: 在記憶存入與搜尋結果中過濾低品質內容

### 過濾類別

| 類別 | 範例 |
|------|------|
| **AI 拒絕回應** | "I don't have any information"、"我沒有相關的資料" |
| **後設問題** | "do you remember"、"你記得嗎" |
| **打招呼/心跳** | "hello"、"HEARTBEAT"、"你好" |
| **過短文本** | 字元數 < 5 |

---

## 7. 自適應檢索 (Adaptive Retrieval)

- **程式檔案**: `internal/memory/adaptive.go`
- **用途**: 自動判斷是否需要記憶搜尋，節省 Embedding API 呼叫並減少噪音注入

### 判定邏輯

```
查詢 → 強制檢索模式？ → [否] → 太短？(<5字元) → [否] → 符合跳過模式？ → [否] → 純Emoji？ → [否] → 短訊息(CJK<6/EN<15)且無問號？ → [否] → 執行檢索
```

| 行為 | 條件 |
|------|------|
| **強制檢索** | 包含記憶關鍵字：「記得」「上次」「之前」「remember」「previously」 |
| **跳過** | 寒暄 (hi/hello/你好)、系統指令 (/start)、確認 (ok/好的)、純 Emoji、HEARTBEAT |
| **CJK 感知** | 中文 6 字元門檻 vs 英文 15 字元門檻 |

---

## 8. 技術改動總覽 (memory-lancedb-pro 移植)

### 新增檔案

| 檔案 | 用途 |
|------|------|
| `internal/memory/scoring.go` | 多階段評分管線 (RetrievalConfig + 7 個評分函式) |
| `internal/memory/noise_filter.go` | 噪音過濾 (英文+中文正則) |
| `internal/memory/adaptive.go` | 自適應檢索 (跳過/強制模式 + CJK 感知) |
| `internal/memory/scoring_test.go` | 14 個單元測試覆蓋所有新功能 |

### 修改檔案

| 檔案 | 變更 |
|------|------|
| `internal/memory/types.go` | 新增 `RetrievalConfig`、`Importance` 欄位、除錯欄位 |
| `internal/memory/search.go` | 整合 `RunScoringPipeline`、RRF 融合改進 |
| `internal/agent/memory_context.go` | 以 `ShouldSkipRetrieval` 取代硬編碼系統指令偵測 |

### 設計決策
- **不引入 LanceDB** — 保持 SQLite 作為唯一儲存後端
- **不引入 Cross-Encoder Reranking** — 避免外部 API 依賴，保持離線優先設計
- **向後相容** — 所有新配置都有合理預設值，無需修改現有設定即可使用

---

## 9. 精準 Token 計算與上下文截斷 (Tiktoken 整合)

為解決單純透過字串長度 (`[]rune`) 估算與截斷可能導致的 Context Window 溢位與不精確問題，系統已整合 `tiktoken-go` 進行精確的 Token 計算。

### 核心組件
- **程式檔案**: `internal/memory/tokens.go`
- **編碼器**: 預設採用 OpenAI 的 `cl100k_base`，與多數主流模型（如 GPT-3.5/GPT-4、Gemini、Ollama等）的 Token 計算邏輯達到更精準的對齊相容。

### 運作機制
1. **分塊精準化 (Indexer)**:
   - 在建立記憶體 (`MEMORY.md` 或其他附加路徑) 的檔案索引塊 (Chunks) 時，原先使用字元長度除以二的粗略估算 (`estimateTokens`)，現已全面替換為 `memory.CountTokens`。
   - 確保寫入 SQLite 索引庫的 Token 數更貼近 LLM 實際消耗量。
2. **上下文檢索截斷 (Context Truncation)**:
   - **短期記憶**: `internal/agent/memory_context.go` 中，近期的歷史紀錄擷取改由 `memory.TruncateByTokens` 進行 **1000 Tokens** 級別的精準截斷。
   - **長期記憶**: 長期搜尋命中片段在混合放入 Prompt 前，透過精確 Token 工具截斷至 **1500 Tokens** (而非原先的字元數)，增加 Context 的利用率並防範 Token 爆掉。
   - **混合搜尋回傳**: 在 `internal/memory/search.go` 中回傳搜尋結果時片段設定的 `MaxSnippetChars`，現已視為 Token 數量上限，並委由 `tiktoken` 處理精確的長度裁切。
3. **退場/容錯機制 (Fallback)**:
   - 考量到特殊環境下 `tiktoken` 編碼器字典檔載入可能遭遇失敗的情境，`tokens.go` 內建自動降級 (Fallback) 至 `utf8.RuneCountInString` 搭配中文字元保守估算法的防禦機制，保障搜尋與 Agent 運行的穩定性不因套件異常而中斷。
